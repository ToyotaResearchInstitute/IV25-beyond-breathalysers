{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run training and validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the bash script: `micromamba run -n IV25 bash run_experiments.sh`. \n",
    "\n",
    "This will run all of the model training/evaluation for the paper in sequence on a single GPU and saves results to `~/wandb`.\n",
    "\n",
    "The experiments use 5-fold cross-validation from 5 different random seeds, so will take a while to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set wandb results path\n",
    "wandb_path = \"~/wandb/\"\n",
    "# all experiments repeated with 5 random seeds\n",
    "random_seed_list = [str(i) for i in range(1, 6)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN vs Foundation Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {}\n",
    "for seed in random_seed_list:\n",
    "    d[seed] = {}\n",
    "    results = aggregate_results(f\"lite{seed}-all-split\", wandb_path)\n",
    "    d[seed]['moment_all'] = get_LR_max_avg_scores(results)\n",
    "    results = aggregate_results(f\"lite{seed}-all-cnn-split\", wandb_path)\n",
    "    d[seed]['cnn_all'] = get_LR_max_avg_scores(results)\n",
    "\n",
    "def aggregate_seed_runs(d):\n",
    "    agg = {}\n",
    "    for seed in d:\n",
    "        for exp in d[seed]:\n",
    "            if exp not in agg.keys():\n",
    "                agg[exp] = {}\n",
    "            for metric in d[seed][exp]:\n",
    "                if metric not in agg[exp].keys():\n",
    "                    agg[exp][metric] = {}\n",
    "                for test in d[seed][exp][metric]:\n",
    "                    if test not in agg[exp][metric].keys():\n",
    "                        agg[exp][metric][test] = {\"results\": []}\n",
    "                    agg[exp][metric][test][\"results\"].append(d[seed][exp][metric][test])\n",
    "    for exp in agg:\n",
    "        for metric in agg[exp]:\n",
    "            for test in agg[exp][metric]:\n",
    "                agg[exp][metric][test][\"mean\"] = np.mean(agg[exp][metric][test][\"results\"])\n",
    "                agg[exp][metric][test][\"std\"] = np.std(agg[exp][metric][test][\"results\"])\n",
    "    return agg\n",
    "\n",
    "agg = aggregate_seed_runs(d)\n",
    "\n",
    "for metric in [\"F1\", \"BAcc\"]:\n",
    "    print(f\"** Mean and std {metric}\")\n",
    "    moment_scores = np.array([agg['moment_all'][metric][ii]['results'] for ii in agg['moment_all'][metric].keys()])\n",
    "    print(f\"MOMENT: {moment_scores.flatten().mean():0.2f} mean, {moment_scores.flatten().std():0.2f} std\")\n",
    "    cnn_scores = np.array([agg['cnn_all'][metric][ii]['results'] for ii in agg['cnn_all'][metric].keys()])\n",
    "    print(f\"CNN: {cnn_scores.flatten().mean():0.2f} mean, {cnn_scores.flatten().std():0.2f} std\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 7(a+b) - F1 and BAcc matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {}\n",
    "for seed in random_seed_list:\n",
    "    d[seed] = {}\n",
    "    for exp in [f\"lite{seed}-gaze_tracking\", f\"lite{seed}-fixed_gaze\", f\"lite{seed}-silent_reading\", f\"lite{seed}-choice_reaction\"]:\n",
    "        results = aggregate_results(exp, wandb_path)\n",
    "        d[seed][exp.split('-')[1]] = get_LR_max_avg_scores(results)\n",
    "    results = aggregate_results(f\"lite{seed}-all-split\", wandb_path)\n",
    "    d[seed]['all'] = get_LR_max_avg_scores(results)\n",
    "\n",
    "agg = aggregate_seed_runs(d)\n",
    "            \n",
    "# Extract keys\n",
    "data_keys = list(agg.keys())\n",
    "metric_keys = [\"F1\", \"BAcc\"]\n",
    "split_keys = [\"gaze_tracking\", \"fixed_gaze\", \"silent_reading\", \"choice_reaction\"]\n",
    "\n",
    "set_plot_style(font_size=16, grid_line_width=\"0.0\")\n",
    "\n",
    "# Set up plots for each metric\n",
    "for metric in metric_keys:\n",
    "    # Prepare data for the heatmap\n",
    "    mean_data = np.array([[agg[data][metric][split][\"mean\"] for split in split_keys] for data in data_keys])\n",
    "    std_data = np.array([[agg[data][metric][split][\"std\"] for split in split_keys] for data in data_keys])\n",
    "    \n",
    "    # Plot matrix\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.imshow(mean_data, cmap='viridis', aspect='auto')\n",
    "    \n",
    "    # Set x and y ticks\n",
    "    plt.xticks(ticks=np.arange(len(split_keys)), labels=[LABEL_TRANSLATE[s] for s in split_keys])\n",
    "    plt.yticks(ticks=np.arange(len(data_keys)), labels=[LABEL_TRANSLATE[s] for s in data_keys])\n",
    "\n",
    "    # Add labels and title\n",
    "    plt.xlabel(\"Test\")\n",
    "    plt.ylabel(\"Training data\")\n",
    "    \n",
    "    # Annotate each cell with the value\n",
    "    for i in range(len(data_keys)):\n",
    "        for j in range(len(split_keys)):\n",
    "            plt.text(j, i, f\"{mean_data[i, j]:.2f} Â± {std_data[i,j]:.2f}\", fontsize=12, ha='center', va='center', color=\"white\" if mean_data[i, j] < 0.5 else \"black\")\n",
    "        \n",
    "    # Display the plot\n",
    "    plt.savefig(f\"../docs/figures/train_vs_test_{metric}_LRmax.pdf\", bbox_inches='tight')\n",
    "    print(f\"{metric}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 7c per subject analysis\n",
    "For all subjects, for one random seed, plot the F1 score on y axis scatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_scores_per_pid(results):\n",
    "    # for each experiment\n",
    "    scores = {}\n",
    "    scores['F1'] = {}\n",
    "    scores['Acc'] = {}\n",
    "    scores['BAcc'] = {}\n",
    "    for exp in results.keys():\n",
    "        scores['F1'][exp] = {}\n",
    "        scores['Acc'][exp] = {}\n",
    "        scores['BAcc'][exp] = {}\n",
    "        for pid in results[exp]:\n",
    "            if pid.startswith(\"P\"):\n",
    "                labels = np.array(results[exp][pid]['labels'])\n",
    "                predicted = np.array(results[exp][pid]['predicted'])\n",
    "\n",
    "                assert(len(labels) % 2 == 0)\n",
    "        \n",
    "                # Reshape to (*, 2) and take max (across L & R eyes)\n",
    "                labels_max = np.max(labels.reshape(-1, 2), axis=1)\n",
    "                labels = labels_max.reshape(-1, 1)\n",
    "        \n",
    "                # Reshape to (*, 2) and take max (across L & R eyes)\n",
    "                predicted_max = np.max(predicted.reshape(-1, 2), axis=1)\n",
    "                predicted = predicted_max.reshape(-1, 1)\n",
    "                        \n",
    "                if sum(labels) == 0 and sum(predicted) == 0:\n",
    "                    tn = len(labels)\n",
    "                    tp, fn, fp = 0, 0, 0\n",
    "                elif sum(labels) == len(labels) and sum(predicted) == len(predicted):\n",
    "                    tp = len(labels)\n",
    "                    tn, fn, fp = 0, 0, 0\n",
    "                else:\n",
    "                    try:\n",
    "                        tn, fp, fn, tp = confusion_matrix(labels, predicted).ravel()\n",
    "                    except:\n",
    "                        print(labels)\n",
    "                        print(predicted)\n",
    "                # print(tn, fp, fn, tp)\n",
    "                scores['F1'][exp][pid] = (2 * tp) / (2 * tp + fp + fn)\n",
    "                scores['Acc'][exp][pid] = (tp + tn) / (tp + tn + fp + fn)\n",
    "                if tp + fn == 0:\n",
    "                    fn = 1\n",
    "                if tn + fp == 0:\n",
    "                    fp = 1            \n",
    "                scores['BAcc'][exp][pid] = 1/2 * tp / (tp + fn) + 1/2 * tn / (tn + fp)            \n",
    "    return scores    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {}\n",
    "for exp in [\"lite1-gaze_tracking\", \"lite1-fixed_gaze\", \"lite1-silent_reading\", \"lite1-choice_reaction\"]:\n",
    "    results = aggregate_results(exp, wandb_path)\n",
    "    d[exp.split('-')[1]] = get_avg_scores_per_pid(results)\n",
    "results = aggregate_results(\"lite1-all-split\", wandb_path)\n",
    "d['all'] = get_avg_scores_per_pid(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract keys\n",
    "data_keys = list(d.keys())\n",
    "metric_keys = [\"F1\", \"BAcc\"]\n",
    "split_keys = [\"gaze_tracking\", \"fixed_gaze\", \"silent_reading\", \"choice_reaction\"]\n",
    "pids = [pid for pid in results[data_keys[0]] if pid.startswith(\"P\")]\n",
    "mean_metrics = {}\n",
    "\n",
    "# Set up plots for each metric\n",
    "for metric in metric_keys:\n",
    "    results_by_pid = {}\n",
    "    for pid in pids:\n",
    "        results_by_pid[pid] = []\n",
    "        for xx in data_keys:\n",
    "            for yy in split_keys:\n",
    "                results_by_pid[pid].append(d[xx][metric][yy][pid])\n",
    "\n",
    "    mean_metrics[metric] = [np.mean(results_by_pid[pid]) for pid in pids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "plt.bar(range(len(mean_metrics[\"F1\"])), sorted(mean_metrics[\"F1\"], reverse=True), zorder=2)\n",
    "plt.xlabel(\"Sorted Participants\")\n",
    "plt.ylabel(\"F1 Score\")\n",
    "plt.ylim((0.5,0.9))\n",
    "set_plot_style(font_size=14, grid_line_width=\"2.0\")\n",
    "plt.grid(True, zorder=1)  \n",
    "plt.savefig(f\"../docs/figures/F1_per_participant.pdf\", bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fig 8a - changing sampling rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "d = {}\n",
    "for seed in random_seed_list:\n",
    "    d[seed] = {}\n",
    "    for rate, exp in zip([\"60\",\"30\",\"20\",\"10\"],[f\"lite{seed}-all-split\", f\"lite{seed}-all-resample30-split\", f\"lite{seed}-all-resample20-split\", f\"lite{seed}-all-resample10-split\"]):\n",
    "        results = aggregate_results(exp, wandb_path)\n",
    "        d[seed][rate] = get_LR_max_avg_scores(results)\n",
    "agg = aggregate_seed_runs(d)\n",
    "\n",
    "F1 = {}\n",
    "BAcc = {}\n",
    "for s in [\"60\",\"30\",\"20\",\"10\"]:\n",
    "    for test in [\"choice_reaction\"]:\n",
    "        if test not in F1:\n",
    "            F1[test] = {\"mean\": [], \"std\": []}\n",
    "        if test not in BAcc:\n",
    "            BAcc[test] = {\"mean\": [], \"std\": []}\n",
    "        F1[test][\"mean\"].append(agg[s][\"F1\"][test][\"mean\"])\n",
    "        F1[test][\"std\"].append(agg[s][\"F1\"][test][\"std\"])\n",
    "        BAcc[test][\"mean\"].append(agg[s][\"BAcc\"][test][\"mean\"])\n",
    "        BAcc[test][\"std\"].append(agg[s][\"BAcc\"][test][\"std\"])   \n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "\n",
    "x = [\"60\", \"30\", \"20\", \"10\"]\n",
    "x_int = list(range(len(x)))\n",
    "\n",
    "def plot_with_bounds(x, values, stds, label, color=None):\n",
    "    plt.plot(x, values, '.-', label=label, color=color)\n",
    "    plt.fill_between(\n",
    "        x,\n",
    "        [v - s for v, s in zip(values, stds)],\n",
    "        [v + s for v, s in zip(values, stds)],\n",
    "        alpha=0.2,\n",
    "        color=color,\n",
    "        zorder=2,\n",
    "    )\n",
    "\n",
    "from itertools import cycle\n",
    "colors = cycle(plt.rcParams['axes.prop_cycle'].by_key()['color'])\n",
    "\n",
    "plot_with_bounds(x, F1[\"choice_reaction\"][\"mean\"], F1[\"choice_reaction\"][\"std\"], \"CR Test F1\", next(colors))\n",
    "plot_with_bounds(x, BAcc[\"choice_reaction\"][\"mean\"], BAcc[\"choice_reaction\"][\"std\"], \"CR Test BAcc\", next(colors))\n",
    "\n",
    "set_plot_style(font_size=14, grid_line_width=\"2.0\")\n",
    "plt.grid(True, zorder=1)  \n",
    "plt.xticks(x_int, x)\n",
    "plt.xlabel(\"Sampling rate (Hz)\")\n",
    "plt.ylabel(\"Test Performance\")\n",
    "plt.ylim((0.4, 0.7))\n",
    "plt.yticks([0.4,0.5,0.6,0.7])\n",
    "plt.legend(ncol=2, loc=\"lower center\")\n",
    "plt.savefig(f\"../docs/figures/changing_S.pdf\", bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 8b - effect of shorter W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "d = {}\n",
    "windows = [8.53,6.4,4.27,2.13]\n",
    "for seed in random_seed_list:\n",
    "    d[seed] = {}\n",
    "    for rate, exp in zip(windows,[f\"lite{seed}-all-split\", f\"lite{seed}-all-window384-split\", f\"lite{seed}-all-window256-split\", f\"lite{seed}-all-window128-split\"]):\n",
    "        results = aggregate_results(exp, wandb_path)\n",
    "        d[seed][rate] = get_LR_max_avg_scores(results)\n",
    "agg = aggregate_seed_runs(d)\n",
    "\n",
    "F1 = {}\n",
    "BAcc = {}\n",
    "for s in windows:\n",
    "    for test in [\"choice_reaction\"]:\n",
    "        if test not in F1:\n",
    "            F1[test] = {\"mean\": [], \"std\": []}\n",
    "        if test not in BAcc:\n",
    "            BAcc[test] = {\"mean\": [], \"std\": []}\n",
    "        F1[test][\"mean\"].append(agg[s][\"F1\"][test][\"mean\"])\n",
    "        F1[test][\"std\"].append(agg[s][\"F1\"][test][\"std\"])\n",
    "        BAcc[test][\"mean\"].append(agg[s][\"BAcc\"][test][\"mean\"])\n",
    "        BAcc[test][\"std\"].append(agg[s][\"BAcc\"][test][\"std\"])   \n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "\n",
    "colors = cycle(plt.rcParams['axes.prop_cycle'].by_key()['color'])\n",
    "\n",
    "plot_with_bounds(windows, F1[\"choice_reaction\"][\"mean\"], F1[\"choice_reaction\"][\"std\"], \"CR Test F1\", next(colors))\n",
    "plot_with_bounds(windows, BAcc[\"choice_reaction\"][\"mean\"], BAcc[\"choice_reaction\"][\"std\"], \"CR Test BAcc\", next(colors))\n",
    "plt.grid(True, zorder=1)  \n",
    "plt.xlabel(\"Input Window (s)\")\n",
    "plt.ylabel(\"Test Performance\")\n",
    "plt.ylim((0.4, 0.7))\n",
    "plt.yticks([0.4,0.5,0.6,0.7])\n",
    "plt.legend(ncol=2, loc=\"lower center\")\n",
    "plt.savefig(f\"../docs/figures/changing_W.pdf\", bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 8c - effect of changing D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {}\n",
    "categories = [\"event\",\"event+gaze\",\"event+gaze+pupil\"]\n",
    "for seed in random_seed_list:\n",
    "    d[seed] = {}\n",
    "    for rate, exp in zip(categories,[f\"lite{seed}-all-event-only-split\", f\"lite{seed}-all-split\", f\"lite{seed}-all-pd-split\"]):\n",
    "        results = aggregate_results(exp, wandb_path)\n",
    "        d[seed][rate] = get_LR_max_avg_scores(results)\n",
    "agg = aggregate_seed_runs(d)\n",
    "\n",
    "F1 = {}\n",
    "BAcc = {}\n",
    "for s in categories:\n",
    "    for test in [\"choice_reaction\"]:\n",
    "        if test not in F1:\n",
    "            F1[test] = {\"mean\": [], \"std\": []}\n",
    "        if test not in BAcc:\n",
    "            BAcc[test] = {\"mean\": [], \"std\": []}\n",
    "        F1[test][\"mean\"].append(agg[s][\"F1\"][test][\"mean\"])\n",
    "        F1[test][\"std\"].append(agg[s][\"F1\"][test][\"std\"])\n",
    "        BAcc[test][\"mean\"].append(agg[s][\"BAcc\"][test][\"mean\"])\n",
    "        BAcc[test][\"std\"].append(agg[s][\"BAcc\"][test][\"std\"])   \n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "\n",
    "colors = cycle(plt.rcParams['axes.prop_cycle'].by_key()['color'])\n",
    "\n",
    "plot_with_bounds(categories, F1[\"choice_reaction\"][\"mean\"], F1[\"choice_reaction\"][\"std\"], \"CR Test F1\", next(colors))\n",
    "plot_with_bounds(categories, BAcc[\"choice_reaction\"][\"mean\"], BAcc[\"choice_reaction\"][\"std\"], \"CR Test BAcc\", next(colors))\n",
    "plt.grid(True, zorder=1)  \n",
    "plt.xlabel(\"Input dimensions\")\n",
    "plt.ylabel(\"Test Performance\")\n",
    "plt.ylim((0.4, 0.7))\n",
    "plt.yticks([0.4,0.5,0.6,0.7])\n",
    "plt.legend(ncol=2, loc=\"lower center\")\n",
    "plt.savefig(f\"../docs/figures/changing_D.pdf\", bbox_inches='tight')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IV25",
   "language": "python",
   "name": "iv25"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
